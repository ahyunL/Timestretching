# UROP - Whisper 모델을 활용한 음성 데이터 시간축 변형 기반데이터 증강 및 음성 인식 성능 평가

## 📌 프로젝트 개요
본 레포지토리는 OpenAI Whisper 모델을 활용한 음성 인식 연구를 위한 코드와 실험 결과를 담고 있습니다.  
특히 시간축 변형(Time-Stretching) 데이터 증강 기법을 적용하여 인식 성능 향상을 평가합니다.

## 🗂️ 폴더 및 파일 구조
- `baseline/` : Whisper 사전학습 모델을 그대로 평가한 기본 코드
- `time_stretch/` : 발화 속도 변형(Time-Stretching) 증강 실험 코드
- `final/` : 최종적으로 정리된 필수 코드 (학습 + 평가 + 결과 출력)

## 📊 결과 요약

본 연구에서는 Whisper 모델을 대상으로 **발화 속도 변형(Time-Stretching)** 데이터 증강 기법을 적용했을 때의 성능 변화를 평가했습니다.  
평가 지표는 **Word Error Rate (WER)** 와 **Character Error Rate (CER)** 를 사용했습니다.

### 1. 모델 크기별 성능 (Baseline)
- 성능은 **Whisper-tiny < base < small** 순서로 나타났다.
  (동일한 베이스라인 코드였으나, base 모델은 수렴 과정에서 덜 수렴되었을 가능성이 있어 추가 튜닝을 진행하였음에도, **small 모델이 더 나은 성능**을 보였다.)  
- 교착어 계열(ko, ja)에서 상대적으로 높은 오류율을 보였다.

### 2. 발화 속도 변형 효과
| Condition        | en    | de    | ko    | ja    |
|------------------|-------|-------|-------|-------|
| Original         | 0.261 | 0.330 | 0.417 | 0.910 |
| 0.9x + orig      | 0.244 | 0.400 | 0.410 | 0.865 |
| 1.1x + orig      | 0.257 | 0.403 | 0.453 | 0.865 |
| All speeds + orig| 0.506 | 0.406 | 0.443 | 0.856 |
| Original x2      | 0.237 | 0.302 | 0.431 | 0.910 |
- **WER**:
- 단일 속도 증강(0.9x, 1.1x)은 일부 언어에서 오차율을 소폭 줄였으나, 전반적으로 일관된 개선은 확인되지 않았다.  
- 모든 속도를 합산한 경우(All speeds)는 오히려 오류율이 증가했다.  
- 단순 데이터 증량(Original ×2)은 특정 언어에서 가장 낮은 WER을 기록하며 긍정적인 효과를 보였다.  


| Condition        | en    | de    | ko    | ja    |
|------------------|-------|-------|-------|-------|
| Original         | 0.127 | 0.126 | 0.162 | 0.521 |
| 0.9x + orig      | 0.119 | 0.162 | 0.160 | 0.432 |
| 1.1x + orig      | 0.129 | 0.172 | 0.183 | 0.355 |
| All speeds + orig| 0.308 | 0.155 | 0.180 | 0.434 |
| Original x2      | 0.117 | 0.113 | 0.195 | 0.342 |
- **CER**:
- CER 기준으로는 0.9x 속도 증강이 전반적으로 개선된 성능을 보였으며, 특히 일본어(ja)에서 오류율이 크게 줄었다.  
- 1.1x 속도 증강 또한 일부 언어에서 긍정적인 영향을 주었다.  
- 그러나 모든 속도 합산(All speeds)은 CER에서도 오히려 성능 저하가 발생했다.  
- 데이터 단순 증량(Original ×2)은 안정적인 개선 효과를 보였다.

### 3) 종합 결론
- **Baseline 비교**: 모델 크기가 커질수록 전반적으로 오류율이 감소하였으나, 학습 안정성에 따라 성능 차이가 발생하였다.  
- **속도 변형 효과**:  
  - 단일 속도 증강은 특정 언어에서 개선 효과가 있었으나, 모든 언어에서 일관된 개선은 확인되지 않았다.  
  - 모든 속도를 동시에 학습한 경우(All speeds + orig)는 오히려 오류율이 증가하는 경향을 보였다.  
  - 데이터 양을 단순히 두 배로 늘린 경우(Original ×2)는 일부 언어에서 긍정적인 효과를 보였다.  
- **언어별 차이**: 한국어·일본어(교착어)에서 여전히 높은 오류율을 보여, 언어 특성과 데이터 분포의 영향이 크다고 판단된다.

👉 결론적으로, **발화 속도 변형은 특정 조건에서 부분적인 성능 개선을 제공하지만, 모든 속도의 단순 합산은 학습 혼란을 유발할 수 있다.** 따라서, 증강은 **선별적 적용**과 **데이터 균형 유지**가 중요하다는 점을 알 수 있다.
