# -*- coding: utf-8 -*-
"""UROP_baselinecode_small.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uNNCKSX3EdRFOssEZYZVrkij16GbqdAr
"""

# í•µì‹¬ íŒ¨í‚¤ì§€(ìŠ¤íŠ¸ë¦¬ë° í˜¸í™˜ ë²„ì „)
!pip -q install "datasets<3.0.0" "evaluate<0.5.0" "transformers>=4.38,<4.45" \
                huggingface_hub torchaudio jiwer soundfile

# fsspec/gcsfs ì¶©ëŒ ì •ë¦¬
!pip -q install -U fsspec==2025.3.0 gcsfs

import datasets, evaluate, transformers, fsspec, gcsfs, sys
print("datasets:", datasets.__version__)    # â† 2.x ì—¬ì•¼ OK
print("evaluate:", evaluate.__version__)    # â† 0.4.x ê¶Œì¥
print("transformers:", transformers.__version__)
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("python:", sys.version)

from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset, Audio, Dataset
from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
from evaluate import load
import random
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union
from evaluate import load as load_metric
import pandas as pd

# 2) ì¬í˜„ì„± ê³ ì •

import os, random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

# ê²°ì •ì  ì—°ì‚°(ê¶Œì¥)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Seed fixed to {}.".format(SEED))

n_train = 200
n_val   = 100
n_test  = 100

langs = ["ko", "ja", "en", "de"]
train_data, val_data, test_data = [], [], []

def take_valid(gen, n):
    out = []
    for ex in gen:
        if ex.get("audio") is not None and ex.get("sentence"):
            out.append(ex)
            if len(out) >= n: break
    return out

for lang in langs:
    print(f"âœ… Loading {lang}...")
    ds_tr = load_dataset("mozilla-foundation/common_voice_16_1", lang,
                         split="train", streaming=True)
    ds_va = load_dataset("mozilla-foundation/common_voice_16_1", lang,
                         split="validation", streaming=True)
    ds_te = load_dataset("mozilla-foundation/common_voice_16_1", lang,
                         split="test", streaming=True)

    train_data += take_valid(ds_tr, n_train)
    val_data   += take_valid(ds_va, n_val)
    test_data  += take_valid(ds_te, n_test)

# ì…”í”Œ(ì‹œë“œ ê³ ì •ë¨)
random.shuffle(train_data); random.shuffle(val_data); random.shuffle(test_data)

train_dataset = Dataset.from_list(train_data)
val_dataset   = Dataset.from_list(val_data)
test_dataset  = Dataset.from_list(test_data)
print({ "train": len(train_dataset), "val": len(val_dataset), "test": len(test_dataset) })

model_checkpoint = "openai/whisper-small"

processor = WhisperProcessor.from_pretrained(model_checkpoint)
feature_extractor = processor.feature_extractor
tokenizer = processor.tokenizer

# 16kHz ìºìŠ¤íŒ…
train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=16000))
val_dataset   = val_dataset.cast_column("audio",   Audio(sampling_rate=16000))
test_dataset  = test_dataset.cast_column("audio",  Audio(sampling_rate=16000))

def prepare_dataset(example):
    audio = example["audio"]
    example["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=16000
    ).input_features[0]
    example["labels"] = processor.tokenizer(example["sentence"]).input_ids
    return example

keep_cols = {"input_features", "labels", "locale"}

processed_dataset_train = train_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in train_dataset.column_names if c not in keep_cols]
)
processed_dataset_val = val_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in val_dataset.column_names if c not in keep_cols]
)
processed_dataset_test = test_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in test_dataset.column_names if c not in keep_cols]
)

processed_dataset_train.set_format(columns=["input_features", "labels", "locale"])
processed_dataset_val.set_format(columns=["input_features", "labels", "locale"])
processed_dataset_test.set_format(columns=["input_features", "labels", "locale"])

import torch
from torch import nn
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint)
# í•™ìŠµ ì¤‘ ìºì‹œ ë¹„í™œì„±(ë©”ëª¨ë¦¬/ì•ˆì •ì„±)
model.config.use_cache = False

from dataclasses import dataclass

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-finetuned",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=5e-5,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=50,
    fp16=torch.cuda.is_available(),
    report_to="none",

    predict_with_generate=True,    # â˜… generate() ê¸°ë°˜ í‰ê°€
    generation_max_length=225,
    # ì•„ë˜ 3ê°œ ì¼œë©´ epochë³„ í‰ê°€/ì €ì¥/ë² ìŠ¤íŠ¸ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥(ê·¸ë˜í”„ìš©)
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset_train,
    eval_dataset=processed_dataset_val,
    tokenizer=processor,
    data_collator=data_collator,
)

trainer.train()

trainer.save_model("./whisper-small-finetuned")
processor.save_pretrained("./whisper-small-finetuned")

# ì–¸ì–´ë³„ ë¶„ë¦¬
val_ko = processed_dataset_val.filter(lambda x: x["locale"] == "ko")
val_en = processed_dataset_val.filter(lambda x: x["locale"] == "en")
val_ja = processed_dataset_val.filter(lambda x: x["locale"] == "ja")
val_de = processed_dataset_val.filter(lambda x: x["locale"] == "de")

lang_datasets = {"ko": val_ko, "en": val_en, "ja": val_ja, "de": val_de}

wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

LANG_NAME = {"ko":"korean", "ja":"japanese", "en":"english", "de":"german"}

lang_scores = {}
for lang, dataset in lang_datasets.items():
    print(f"\nğŸŒ Language: {lang.upper()}")
    # ì–¸ì–´ ê°•ì œ í”„ë¡¬í”„íŠ¸ â†’ ì¸í¼ëŸ°ìŠ¤ ì•ˆì •í™”
    forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
    trainer.model.generation_config.forced_decoder_ids = forced_ids
    trainer.model.generation_config.max_length = 225

    pred_output = trainer.predict(dataset)  # generate ê²°ê³¼ ë°˜í™˜
    decoded_preds = processor.tokenizer.batch_decode(pred_output.predictions, skip_special_tokens=True)
    decoded_labels = processor.tokenizer.batch_decode(pred_output.label_ids,   skip_special_tokens=True)

    # ìƒ˜í”Œ ì¶œë ¥
    for p, r in list(zip(decoded_preds, decoded_labels))[:3]:
        print(f"ğŸ”¹Pred: {p}\nğŸ”¸Ref : {r}\n---")

    wer_score = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)
    cer_score = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)
    lang_scores[lang] = {"WER": wer_score, "CER": cer_score, "N": len(decoded_labels)}

    print(f"âœ… {lang.upper()} WER: {wer_score:.4f}")
    print(f"âœ… {lang.upper()} CER: {cer_score:.4f}")

import pandas as pd
df_base = pd.DataFrame(
    [{"lang": k, "WER": v["WER"], "CER": v["CER"], "N": v["N"]} for k, v in lang_scores.items()]
).sort_values("lang")
display(df_base)
df_base.to_csv("/content/baseline_cv16_1_whisper_small.csv", index=False)
print("Saved -> /content/baseline_cv16_1_whisper_small.csv")

import matplotlib.pyplot as plt
import numpy as np

# âœ… ì–¸ì–´ë³„ ê²°ê³¼ (ì˜ˆì‹œê°’ - ë„ˆ ê²°ê³¼ì— ë§ê²Œ ìˆ˜ì •í•´ì¤˜)
languages = ["Korean", "Japanese", "English", "German"]
wer = [0.32, 0.96, 0.19, 0.20]   # ì–¸ì–´ë³„ WER
cer = [0.12, 0.43, 0.08, 0.06]   # ì–¸ì–´ë³„ CER

x = np.arange(len(languages))  # ì–¸ì–´ ê°œìˆ˜ë§Œí¼ ìœ„ì¹˜
width = 0.35  # ë§‰ëŒ€ ë„ˆë¹„

plt.figure(figsize=(8,6))

# âœ… WER ë§‰ëŒ€
plt.bar(x - width/2, wer, width, label="WER", color="#6A5ACD")

# âœ… CER ë§‰ëŒ€
plt.bar(x + width/2, cer, width, label="CER", color="#ff7f0e")

# âœ… ì¶• & ì œëª©
plt.xticks(x, languages, fontsize=12)
plt.ylabel("Error Rate", fontsize=12)
plt.title("Language-wise WER vs CER", fontsize=14, fontweight="bold")
plt.legend()

# âœ… ìˆ˜ì¹˜ ë¼ë²¨ í‘œì‹œ
for i in range(len(languages)):
    plt.text(x[i] - width/2, wer[i] + 0.01, f"{wer[i]:.2f}", ha="center", fontsize=10)
    plt.text(x[i] + width/2, cer[i] + 0.01, f"{cer[i]:.2f}", ha="center", fontsize=10)

plt.tight_layout()
plt.show()