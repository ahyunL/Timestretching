# -*- coding: utf-8 -*-
"""UROP_TimeStretch(원본+0.7)_small.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mwnhZCS7nM5afM_iIqZ7_9M-W2NOntDa
"""

# 핵심 패키지(스트리밍 호환 버전)
!pip -q install "datasets<3.0.0" "evaluate<0.5.0" "transformers>=4.38,<4.45" \
                huggingface_hub torchaudio jiwer soundfile

# fsspec/gcsfs 충돌 정리
!pip -q install -U fsspec==2025.3.0 gcsfs

import datasets, evaluate, transformers, fsspec, gcsfs, sys
print("datasets:", datasets.__version__)    # ← 2.x 여야 OK
print("evaluate:", evaluate.__version__)    # ← 0.4.x 권장
print("transformers:", transformers.__version__)
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("python:", sys.version)

from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset, Audio, Dataset, concatenate_datasets
from transformers import (
    WhisperProcessor, WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback
)
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from evaluate import load as load_metric
import pandas as pd

import random
import torch

# 2) 재현성 고정

import os, random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

# 결정적 연산(권장)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Seed fixed to {}.".format(SEED))

n_train, n_val, n_test = 200, 100, 100
langs = ["ko","ja","en","de"]

def take_valid(gen, n):
    out = []
    for ex in gen:
        if ex.get("audio") is not None and ex.get("sentence"):
            out.append(ex)
            if len(out) >= n: break
    return out

train_data, val_data, test_data = [], [], []
for lang in langs:
    print(f"✅ Loading {lang}...")
    ds_tr = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="train",       streaming=True)
    ds_va = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="validation",  streaming=True)
    ds_te = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="test",        streaming=True)
    train_data += take_valid(ds_tr, n_train)
    val_data   += take_valid(ds_va, n_val)
    test_data  += take_valid(ds_te, n_test)

random.shuffle(train_data); random.shuffle(val_data); random.shuffle(test_data)
train_dataset = Dataset.from_list(train_data)
val_dataset   = Dataset.from_list(val_data)
test_dataset  = Dataset.from_list(test_data)
print({"train": len(train_dataset), "val": len(val_dataset), "test": len(test_dataset)})

TARGET_SR = 16000
model_checkpoint = "openai/whisper-small"

processor = WhisperProcessor.from_pretrained(model_checkpoint)
feature_extractor = processor.feature_extractor
tokenizer = processor.tokenizer

train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
val_dataset   = val_dataset.cast_column("audio",   Audio(sampling_rate=TARGET_SR))
test_dataset  = test_dataset.cast_column("audio",  Audio(sampling_rate=TARGET_SR))

def prepare_dataset(example):
    a = example["audio"]
    example["input_features"] = processor.feature_extractor(a["array"], sampling_rate=TARGET_SR).input_features[0]
    example["labels"] = processor.tokenizer(example["sentence"]).input_ids
    return example

keep = {"input_features","labels","locale"}
proc_train_orig = train_dataset.map(prepare_dataset, remove_columns=[c for c in train_dataset.column_names if c not in keep])
proc_val_orig   = val_dataset.map(prepare_dataset,   remove_columns=[c for c in val_dataset.column_names   if c not in keep])
proc_test_orig  = test_dataset.map(prepare_dataset,  remove_columns=[c for c in test_dataset.column_names  if c not in keep])

proc_train_orig.set_format(columns=list(keep))
proc_val_orig.set_format(columns=list(keep))
proc_test_orig.set_format(columns=list(keep))

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device, "| Seed:", SEED)

model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint).to(device)
model.config.use_cache = False  # 학습 안정성
# 학습 중 언어 강제 끄기 (중요)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

!apt-get -y install sox libsox-dev libsox-fmt-all

import numpy as np, torchaudio, torch

def _to_tensor_1ch(x_np):
    x = torch.tensor(np.asarray(x_np, dtype=np.float32))
    if x.dim() == 1:
        x = x.unsqueeze(0)
    return x

def speed_perturb_np(wav_np, sr, rate: float):
    """
    함수명은 그대로 두고, 내부는 tempo(피치 보존)로 동작.
    """
    if rate == 1.0:
        return wav_np, sr
    wav = _to_tensor_1ch(wav_np)
    effects = [["tempo", str(rate)]]  # ← CHANGED: speed+rate -> tempo(피치 보존)
    aug, sr_out = torchaudio.sox_effects.apply_effects_tensor(wav, sr, effects)
    return aug.squeeze(0).numpy(), sr_out

# 원본 raw(16k 보장)로부터 증강 사본 생성 → prepare_dataset 재사용
train_raw_16k = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))

def augment_copy(ds_raw, rate: float):
    def _aug(b):
        a = b["audio"]
        aug_np, sr_out = speed_perturb_np(a["array"], a["sampling_rate"], rate)
        b["audio"] = {"array": aug_np, "path": None, "sampling_rate": sr_out}
        return b
    ds_aug = ds_raw.map(_aug)
    ds_proc = ds_aug.map(
        prepare_dataset,
        remove_columns=[c for c in ds_aug.column_names if c not in {"input_features","labels","locale"}]
    )
    ds_proc.set_format(columns=["input_features","labels","locale"])
    return ds_proc

# ★ 이번 실험: 원본 + 0.7 (1:1)
proc_train_sp07 = augment_copy(train_raw_16k, 0.7)

# 원본 길이에 맞춰 1:1로 합치기  (❗select에 정수 대신 range 사용)
k = len(proc_train_orig)
m = min(len(proc_train_sp07), k)  # 사용할 증강 샘플 수
proc_train_sp07_sel = proc_train_sp07.shuffle(seed=SEED).select(range(m))

# 원본 전체 + 증강 m개 (원본이 더 많아도 OK)
proc_train_mix = concatenate_datasets([proc_train_orig, proc_train_sp07_sel]).shuffle(seed=SEED)

print(
    "Train(orig):", len(proc_train_orig),
    "Train(0.7 sel):", len(proc_train_sp07_sel),
    "→ Mixed:", len(proc_train_mix)
)

# ---- 모델/생성 설정 (Trainer 전에) ----
start_id = processor.tokenizer.convert_tokens_to_ids("<|startoftranscript|>")
model.config.decoder_start_token_id = start_id
model.generation_config.decoder_start_token_id = start_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.use_cache = False
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback

args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-mix_0p7",   # ← CHANGED
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2e-5,
    num_train_epochs=8,
    weight_decay=0.01,
    label_smoothing_factor=0.1,     # 유지(원하면 0.05~0.1 범위 튜닝)
    warmup_ratio=0.1,
    lr_scheduler_type="linear",
    max_grad_norm=1.0,
    fp16=torch.cuda.is_available(),
    report_to="none",

    predict_with_generate=True,
    generation_max_length=225,

    eval_strategy="epoch",          # ← deprecation 대응
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    remove_unused_columns=False,    # ← 중요: input_features 보존
)

trainer_mix07 = Seq2SeqTrainer(   # ← CHANGED
    model=model,
    args=args,
    train_dataset=proc_train_mix,
    eval_dataset=proc_val_orig,
    tokenizer=processor,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

LANG_NAME = {"ko":"korean", "ja":"japanese", "en":"english", "de":"german"}
wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

# (A) 평가 함수 교체: 샘플 출력 + label_ids 안전 디코딩
def eval_on(ds_proc, trainer_obj, label, sample_k=10, show_samples=True):
    rows = []
    for lang in ["ko","ja","en","de"]:
        subset = ds_proc.filter(lambda x: x["locale"] == lang)

        # 평가 시 언어 프롬프트 강제
        forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
        trainer_obj.model.generation_config.forced_decoder_ids = forced_ids
        trainer_obj.model.generation_config.max_length = 225

        out = trainer_obj.predict(subset)

        # 예측/정답 디코딩 (−100 마스크를 pad 토큰으로 치환 후 디코딩)
        preds = processor.tokenizer.batch_decode(out.predictions, skip_special_tokens=True)
        label_ids = np.where(out.label_ids != -100, out.label_ids, processor.tokenizer.pad_token_id)
        refs  = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

        # 지표
        wer = wer_metric.compute(predictions=preds, references=refs)
        cer = cer_metric.compute(predictions=preds, references=refs)
        rows.append({"set": label, "lang": lang, "WER": wer, "CER": cer, "N": len(refs)})

        # 샘플 k개 출력
        if show_samples and sample_k > 0:
            print(f"\n[{label}] {lang.upper()} samples (k={sample_k})")
            for i, (p, r) in enumerate(zip(preds, refs)):
                if i >= sample_k: break
                print(f"#{i:02d} PRED: {p}")
                print(f"     REF : {r}")
                print("---")

    return pd.DataFrame(rows).sort_values("lang")

# 호출부만 k 지정해주면 됨
df_orig = eval_on(proc_test_orig, trainer_mix07, "test_orig", sample_k=10, show_samples=True)

# (B) 0.7배로 변형된 test 생성 → 평가 (robustness 참고용)
test_raw_16k = test_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
def make_processed_with_rate(ds_raw, rate: float):
    def _aug(b):
        a = b["audio"]
        aug_np, sr_out = speed_perturb_np(a["array"], a["sampling_rate"], rate)
        b["audio"] = {"array": aug_np, "path": None, "sampling_rate": sr_out}
        return b
    ds_aug = ds_raw.map(_aug)
    ds_proc = ds_aug.map(prepare_dataset, remove_columns=[c for c in ds_aug.column_names if c not in {"input_features","labels","locale"}])
    ds_proc.set_format(columns=["input_features","labels","locale"])
    return ds_proc

proc_test_r07 = make_processed_with_rate(test_raw_16k, 0.7)
df_r07   = eval_on(proc_test_r07, trainer_mix07, "test_rate0.7", sample_k=10, show_samples=True)

display(df_orig, df_r07)
df_all = pd.concat([df_orig, df_r07], ignore_index=True)
df_all.to_csv("/content/results_mix0p7_test_orig_and_r07.csv", index=False)
print("Saved -> /content/results_mix0p7_test_orig_and_r07.csv")

import matplotlib.pyplot as plt

def bar_lang(df, label):
    plt.figure(figsize=(7,5))
    langs = ["ko","ja","en","de"]
    vals = [df[df["lang"]==l]["WER"].values[0] for l in langs]
    plt.bar([l.upper() for l in langs], vals)
    plt.title(f"WER - {label}"); plt.ylabel("WER"); plt.tight_layout(); plt.show()

bar_lang(df_orig, "Original Test")
bar_lang(df_r07,  "Rate 0.7 Test")   # ← CHANGED

# 학습 직전 sanity check
print("train mix size:", len(proc_train_mix))
print("val (orig) size:", len(proc_val_orig))
print("first keys train:", proc_train_mix.features)
print("training on:", trainer_mix07.args.output_dir)  # ← CHANGED

# 반드시 train_dataset가 proc_train_mix 인지 확인
assert trainer_mix07.train_dataset is proc_train_mix  # ← CHANGED
# 검증은 원본만 쓰는지 확인
assert trainer_mix07.eval_dataset is proc_val_orig    # ← CHANGED

# 언어별로 현재 test_dataset 상위 3문장 확인
for lang in ["ko","ja","en","de"]:
    subset = test_dataset.filter(lambda x: x["locale"] == lang)
    print(f"\n[TEST RAW] {lang.upper()} first 3 sentences")
    for ex in subset.select(range(min(3, len(subset)))):
        print(" -", ex["sentence"][:120])