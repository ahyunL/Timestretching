# -*- coding: utf-8 -*-
"""UROP_TimeStretch(원본+0.9)_small.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11vnexs3j5D0wqvytDMPLHihwMQ7XCCxc
"""

# 핵심 패키지(스트리밍 호환 버전)
!pip -q install "datasets<3.0.0" "evaluate<0.5.0" "transformers>=4.38,<4.45" \
                huggingface_hub torchaudio jiwer soundfile

# fsspec/gcsfs 충돌 정리
!pip -q install -U fsspec==2025.3.0 gcsfs

import datasets, evaluate, transformers, fsspec, gcsfs, sys
print("datasets:", datasets.__version__)    # ← 2.x 여야 OK
print("evaluate:", evaluate.__version__)    # ← 0.4.x 권장
print("transformers:", transformers.__version__)
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("python:", sys.version)

from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset, Audio, Dataset, concatenate_datasets
from transformers import (
    WhisperProcessor, WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback
)
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from evaluate import load as load_metric
from IPython.display import display
import pandas as pd
import matplotlib.pyplot as plt

import os, random, numpy as np, torch

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
print("Seed fixed to {}.".format(SEED))

# 2) 데이터 로드(Streaming -> in-memory)
n_train, n_val, n_test = 200, 100, 100
langs = ["ko","ja","en","de"]

def take_valid(gen, n, lang_code):
    out = []
    for ex in gen:
        if ex.get("audio") is not None and ex.get("sentence"):
            ex = dict(ex)
            ex["locale"] = lang_code
            out.append(ex)
            if len(out) >= n:
                break
    return out

train_data, val_data, test_data = [], [], []
for lang in langs:
    print(f"✅ Loading {lang}...")
    ds_tr = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="train",      streaming=True)
    ds_va = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="validation", streaming=True)
    ds_te = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="test",       streaming=True)
    train_data += take_valid(ds_tr, n_train, lang)
    val_data   += take_valid(ds_va, n_val,   lang)
    test_data  += take_valid(ds_te, n_test,  lang)

random.shuffle(train_data); random.shuffle(val_data); random.shuffle(test_data)
train_dataset = Dataset.from_list(train_data)
val_dataset   = Dataset.from_list(val_data)
test_dataset  = Dataset.from_list(test_data)
print({"train": len(train_dataset), "val": len(val_dataset), "test": len(test_dataset)})

# 3) 전처리
TARGET_SR = 16000
model_checkpoint = "openai/whisper-small"

processor = WhisperProcessor.from_pretrained(model_checkpoint)
feature_extractor = processor.feature_extractor
tokenizer = processor.tokenizer

train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
val_dataset   = val_dataset.cast_column("audio",   Audio(sampling_rate=TARGET_SR))
test_dataset  = test_dataset.cast_column("audio",  Audio(sampling_rate=TARGET_SR))

def prepare_dataset(example):
    a = example["audio"]
    example["input_features"] = feature_extractor(a["array"], sampling_rate=TARGET_SR).input_features[0]
    example["labels"] = tokenizer(example["sentence"]).input_ids
    return example

keep_cols = {"input_features","labels","locale"}
proc_train_orig = train_dataset.map(prepare_dataset, remove_columns=[c for c in train_dataset.column_names if c not in keep_cols])
proc_val_orig   = val_dataset.map(  prepare_dataset, remove_columns=[c for c in val_dataset.column_names   if c not in keep_cols])
proc_test_orig  = test_dataset.map( prepare_dataset, remove_columns=[c for c in test_dataset.column_names  if c not in keep_cols])

for ds_ in (proc_train_orig, proc_val_orig, proc_test_orig):
    ds_.set_format(columns=list(keep_cols))

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device, "| Seed:", SEED)

# 4) 모델 로딩 & Whisper generate() 안전 세팅
model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint).to(device)

# pad/eos/start/forced 프롬프트 확정
tok = processor.tokenizer
if tok.pad_token_id is None:
    tok.pad_token = tok.eos_token

start_id = tok.convert_tokens_to_ids("<|startoftranscript|>")
if start_id is None:
    start_id = processor.get_decoder_prompt_ids(language="english", task="transcribe")[0][1]

forced_ids_en = processor.get_decoder_prompt_ids(language="english", task="transcribe")

for cfg in (model.config, model.generation_config):
    cfg.pad_token_id = tok.pad_token_id
    cfg.eos_token_id = tok.eos_token_id
    cfg.decoder_start_token_id = start_id
    cfg.forced_decoder_ids = forced_ids_en

model.config.use_cache = False
model.config.suppress_tokens = []

# 5) Collator: 라벨 패딩 & decoder_input_ids 생성(우리가 명시 제공)
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # 입력 피처
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # 라벨 패딩 + -100 마스킹
        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels.contiguous()

        # decoder_input_ids (right-shift with <|startoftranscript|>)
        tok = self.processor.tokenizer
        start_id = tok.convert_tokens_to_ids("<|startoftranscript|>")
        if start_id is None:
            start_id = self.processor.get_decoder_prompt_ids(language="english", task="transcribe")[0][1]
        pad_id = tok.pad_token_id

        labels_clean = labels.clone()
        labels_clean[labels_clean == -100] = pad_id

        dec_inp = torch.full_like(labels_clean, fill_value=pad_id)
        dec_inp[:, 0] = start_id
        dec_inp[:, 1:] = labels_clean[:, :-1]

        batch["decoder_input_ids"] = dec_inp.contiguous()
        batch["decoder_attention_mask"] = (dec_inp != pad_id).long()
        batch["input_features"] = batch["input_features"].contiguous()
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

!apt-get -y install sox libsox-dev libsox-fmt-all

import numpy as np, torchaudio

def _to_tensor_1ch(x_np):
    x = torch.tensor(np.asarray(x_np, dtype=np.float32))
    if x.dim() == 1:
        x = x.unsqueeze(0)
    return x

def speed_perturb_np(wav_np, sr, rate: float):
    if rate == 1.0:
        return wav_np, sr
    try:
        wav = _to_tensor_1ch(wav_np)
        effects = [["tempo", str(rate)]]  # 피치 보존
        aug, sr_out = torchaudio.sox_effects.apply_effects_tensor(wav, sr, effects)
        return aug.squeeze(0).numpy(), sr_out
    except Exception as e:
        print("[WARN][sox] tempo failed -> fallback(no-aug):", e)
        return wav_np, sr

train_raw_16k = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))

def augment_copy(ds_raw, rate: float):
    def _aug(b):
        a = b["audio"]
        aug_np, sr_out = speed_perturb_np(a["array"], a["sampling_rate"], rate)
        b = dict(b)
        b["audio"] = {"array": aug_np, "path": None, "sampling_rate": sr_out}
        return b
    ds_aug = ds_raw.map(_aug)
    ds_proc = ds_aug.map(
        prepare_dataset,
        remove_columns=[c for c in ds_aug.column_names if c not in {"input_features","labels","locale"}]
    )
    ds_proc.set_format(columns=["input_features","labels","locale"])
    return ds_proc

# 원본 + 0.9배 속도 증강 샘플 1:1
proc_train_sp09 = augment_copy(train_raw_16k, 0.9)
k = len(proc_train_orig)
m = min(len(proc_train_sp09), k)
proc_train_sp09_sel = proc_train_sp09.shuffle(seed=SEED).select(range(m))
proc_train_mix = concatenate_datasets([proc_train_orig, proc_train_sp09_sel]).shuffle(seed=SEED)
print("Train(orig):", len(proc_train_orig), "Train(0.9 sel):", len(proc_train_sp09_sel), "→ Mixed:", len(proc_train_mix))

# 7) Trainer 설정
args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-mix_0p9",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2e-5,
    num_train_epochs=8,
    weight_decay=0.01,
    label_smoothing_factor=0.1,
    warmup_ratio=0.1,
    lr_scheduler_type="linear",
    max_grad_norm=1.0,
    fp16=torch.cuda.is_available(),
    report_to="none",

    predict_with_generate=True,      # eval/predict 시 generate 사용
    generation_max_length=225,

    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    remove_unused_columns=False,
)

trainer_mix09 = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=proc_train_mix,
    eval_dataset=proc_val_orig,
    tokenizer=processor.tokenizer,   # 중요!
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

print("[sanity] before train:", trainer_mix09.state.global_step)
trainer_mix09.train()
print("[sanity] after  train:", trainer_mix09.state.global_step)

# 8) 평가
LANG_NAME = {"ko":"korean", "ja":"japanese", "en":"english", "de":"german"}
wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

def eval_on(ds_proc, trainer_obj, label, sample_k=10, show_samples=True):
    rows = []
    for lang in ["ko","ja","en","de"]:
        subset = ds_proc.filter(lambda x: x["locale"] == lang)
        if len(subset) == 0:
            print(f"[WARN] No samples for {lang} in {label}")
            rows.append({"set": label, "lang": lang, "WER": float("nan"), "CER": float("nan"), "N": 0})
            continue

        # 언어별 프롬프트 강제 (generate 시 사용)
        forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
        trainer_obj.model.generation_config.forced_decoder_ids = forced_ids
        trainer_obj.model.generation_config.max_length = 225

        out = trainer_obj.predict(subset)  # predict_with_generate=True → 토큰 시퀀스 반환

        # 예측/정답 디코딩
        preds = tokenizer.batch_decode(out.predictions, skip_special_tokens=True)
        label_ids = np.where(out.label_ids != -100, out.label_ids, tokenizer.pad_token_id)
        refs  = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

        wer = wer_metric.compute(predictions=preds, references=refs)
        cer = cer_metric.compute(predictions=preds, references=refs)
        rows.append({"set": label, "lang": lang, "WER": wer, "CER": cer, "N": len(refs)})

        if show_samples and sample_k > 0:
            print(f"\n[{label}] {lang.upper()} samples (k={min(sample_k, len(refs))})")
            for i, (p, r) in enumerate(zip(preds, refs)):
                if i >= sample_k: break
                print(f"#{i:02d} PRED: {p}")
                print(f"     REF : {r}")
                print("---")

    return pd.DataFrame(rows).sort_values("lang")

print(f"[CHECK] Evaluating on TEST set: size={len(proc_test_orig)} (val={len(proc_val_orig)})")
df_orig = eval_on(proc_test_orig, trainer_mix09, "test_orig", sample_k=10, show_samples=True)
display(df_orig)
df_orig.to_csv("/content/results_mix0p9_test_orig_ONLY.csv", index=False)
print("Saved -> /content/results_mix0p9_test_orig_ONLY.csv")

import matplotlib.pyplot as plt

# 9) 시각화
def bar_lang(df, label):
    plt.figure(figsize=(7,5))
    langs = ["ko","ja","en","de"]
    vals = []
    for l in langs:
        row = df[df["lang"]==l]
        vals.append(float(row["WER"].values[0]) if len(row) else float("nan"))
    plt.bar([l.upper() for l in langs], vals)
    plt.title(f"WER - {label}")
    plt.ylabel("WER")
    plt.tight_layout()
    plt.show()

bar_lang(df_orig, "Original Test (mix 0.9)")
