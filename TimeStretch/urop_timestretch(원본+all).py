# -*- coding: utf-8 -*-
"""UROP_TimeStretch(원본+all).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iVxrGLRkqB7UuyNNnL2jiBnhvPZ23n1h
"""

# 핵심 패키지(스트리밍 호환 버전)
!pip -q install "datasets<3.0.0" "evaluate<0.5.0" "transformers>=4.38,<4.45" \
                huggingface_hub torchaudio jiwer soundfile

# fsspec/gcsfs 충돌 정리
!pip -q install -U fsspec==2025.3.0 gcsfs

import datasets, evaluate, transformers, fsspec, gcsfs, sys
print("datasets:", datasets.__version__)    # ← 2.x 여야 OK
print("evaluate:", evaluate.__version__)    # ← 0.4.x 권장
print("transformers:", transformers.__version__)
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("python:", sys.version)

from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset, Audio, Dataset, concatenate_datasets
from transformers import (
    WhisperProcessor, WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback
)
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from evaluate import load as load_metric
import pandas as pd

import random
import torch

# 2) 재현성 고정
import os, random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

# 결정적 연산(권장)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Seed fixed to {}.".format(SEED))

n_train, n_val, n_test = 200, 100, 100
langs = ["ko","ja","en","de"]

def take_valid(gen, n):
    out = []
    for ex in gen:
        if ex.get("audio") is not None and ex.get("sentence"):
            out.append(ex)
            if len(out) >= n: break
    return out

train_data, val_data, test_data = [], [], []
for lang in langs:
    print(f"✅ Loading {lang}...")
    ds_tr = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="train",       streaming=True)
    ds_va = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="validation",  streaming=True)
    ds_te = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="test",        streaming=True)
    train_data += take_valid(ds_tr, n_train)
    val_data   += take_valid(ds_va, n_val)
    test_data  += take_valid(ds_te, n_test)

random.shuffle(train_data); random.shuffle(val_data); random.shuffle(test_data)
train_dataset = Dataset.from_list(train_data)
val_dataset   = Dataset.from_list(val_data)
test_dataset  = Dataset.from_list(test_data)
print({"train": len(train_dataset), "val": len(val_dataset), "test": len(test_dataset)})

TARGET_SR = 16000
model_checkpoint = "openai/whisper-tiny"

processor = WhisperProcessor.from_pretrained(model_checkpoint)
feature_extractor = processor.feature_extractor
tokenizer = processor.tokenizer

train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
val_dataset   = val_dataset.cast_column("audio",   Audio(sampling_rate=TARGET_SR))
test_dataset  = test_dataset.cast_column("audio",  Audio(sampling_rate=TARGET_SR))

def prepare_dataset(example):
    a = example["audio"]
    example["input_features"] = processor.feature_extractor(a["array"], sampling_rate=TARGET_SR).input_features[0]
    example["labels"] = processor.tokenizer(example["sentence"]).input_ids
    return example

keep = {"input_features","labels","locale"}
proc_train_orig = train_dataset.map(prepare_dataset, remove_columns=[c for c in train_dataset.column_names if c not in keep])
proc_val_orig   = val_dataset.map(prepare_dataset,   remove_columns=[c for c in val_dataset.column_names   if c not in keep])
proc_test_orig  = test_dataset.map(prepare_dataset,  remove_columns=[c for c in test_dataset.column_names  if c not in keep])

proc_train_orig.set_format(columns=list(keep))
proc_val_orig.set_format(columns=list(keep))
proc_test_orig.set_format(columns=list(keep))

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device, "| Seed:", SEED)

model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint).to(device)
model.config.use_cache = False  # 학습 안정성
# 학습 중 언어 강제 끄기 (중요)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

!apt-get -y install sox libsox-dev libsox-fmt-all

import numpy as np, torchaudio, torch

def _to_tensor_1ch(x_np):
    x = torch.tensor(np.asarray(x_np, dtype=np.float32))
    if x.dim() == 1:
        x = x.unsqueeze(0)
    return x

def speed_perturb_np(wav_np, sr, rate: float):
    """
    함수명은 그대로 두고, 내부는 tempo(피치 보존)로 동작.
    """
    if rate == 1.0:
        return wav_np, sr
    wav = _to_tensor_1ch(wav_np)
    effects = [["tempo", str(rate)]]  # 피치 보존
    aug, sr_out = torchaudio.sox_effects.apply_effects_tensor(wav, sr, effects)
    return aug.squeeze(0).numpy(), sr_out

# 원본 raw(16k 보장)로부터 증강 사본 생성 → prepare_dataset 재사용
train_raw_16k = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))

def augment_copy(ds_raw, rate: float):
    def _aug(b):
        a = b["audio"]
        aug_np, sr_out = speed_perturb_np(a["array"], a["sampling_rate"], rate)
        b["audio"] = {"array": aug_np, "path": None, "sampling_rate": sr_out}
        return b
    ds_aug = ds_raw.map(_aug)
    ds_proc = ds_aug.map(
        prepare_dataset,
        remove_columns=[c for c in ds_aug.column_names if c not in {"input_features","labels","locale"}]
    )
    ds_proc.set_format(columns=["input_features","labels","locale"])
    return ds_proc

# ★ 이번 실험: 원본 + (0.7, 0.9, 1.1, 1.3) 각각 200개씩 = 800개
proc_train_sp07 = augment_copy(train_raw_16k, 0.7)
proc_train_sp09 = augment_copy(train_raw_16k, 0.9)
proc_train_sp11 = augment_copy(train_raw_16k, 1.1)
proc_train_sp13 = augment_copy(train_raw_16k, 1.3)

# 원본 길이에 맞춰 각 증강도 동일 수량 선택 (1:1:1:1:1)
k = len(proc_train_orig)  # 200
m07 = min(len(proc_train_sp07), k)
m09 = min(len(proc_train_sp09), k)
m11 = min(len(proc_train_sp11), k)
m13 = min(len(proc_train_sp13), k)

proc_train_sp07_sel = proc_train_sp07.shuffle(seed=SEED).select(range(m07))
proc_train_sp09_sel = proc_train_sp09.shuffle(seed=SEED).select(range(m09))
proc_train_sp11_sel = proc_train_sp11.shuffle(seed=SEED).select(range(m11))
proc_train_sp13_sel = proc_train_sp13.shuffle(seed=SEED).select(range(m13))

# 원본(200) + 증강 4종(각 200) = 총 1000
proc_train_mix = concatenate_datasets([
    proc_train_orig,
    proc_train_sp07_sel, proc_train_sp09_sel,
    proc_train_sp11_sel, proc_train_sp13_sel
]).shuffle(seed=SEED)

print(
    "Train(orig):", len(proc_train_orig),
    "Train(0.7 sel):", len(proc_train_sp07_sel),
    "Train(0.9 sel):", len(proc_train_sp09_sel),
    "Train(1.1 sel):", len(proc_train_sp11_sel),
    "Train(1.3 sel):", len(proc_train_sp13_sel),
    "→ Mixed:", len(proc_train_mix)
)

# ---- 모델/생성 설정 (Trainer 전에) ----
start_id = processor.tokenizer.convert_tokens_to_ids("<|startoftranscript|>")
model.config.decoder_start_token_id = start_id
model.generation_config.decoder_start_token_id = start_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.use_cache = False
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback

args = Seq2SeqTrainingArguments(
    output_dir="./whisper-tiny-mix_all4",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2e-5,
    num_train_epochs=8,
    weight_decay=0.01,
    label_smoothing_factor=0.1,     # 필요 시 0.05~0.1 조정
    warmup_ratio=0.1,
    lr_scheduler_type="linear",
    max_grad_norm=1.0,
    fp16=torch.cuda.is_available(),
    report_to="none",

    predict_with_generate=True,
    generation_max_length=225,

    eval_strategy="epoch",          # deprecation 대응
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    remove_unused_columns=False,    # 중요: input_features 보존
)

trainer_mix_all4 = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=proc_train_mix,
    eval_dataset=proc_val_orig,
    tokenizer=processor,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

LANG_NAME = {"ko":"korean", "ja":"japanese", "en":"english", "de":"german"}
wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

# (A) 원본 test 평가 (보고용 공식 지표)
def eval_on(ds_proc, trainer_obj, label):
    rows = []
    for lang in ["ko","ja","en","de"]:
        subset = ds_proc.filter(lambda x: x["locale"] == lang)
        forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
        trainer_obj.model.generation_config.forced_decoder_ids = forced_ids
        trainer_obj.model.generation_config.max_length = 225
        out = trainer_obj.predict(subset)
        preds = processor.tokenizer.batch_decode(out.predictions, skip_special_tokens=True)
        refs  = processor.tokenizer.batch_decode(out.label_ids,   skip_special_tokens=True)
        wer = wer_metric.compute(predictions=preds, references=refs)
        cer = cer_metric.compute(predictions=preds, references=refs)
        rows.append({"set": label, "lang": lang, "WER": wer, "CER": cer, "N": len(refs)})
    return pd.DataFrame(rows).sort_values("lang")

df_orig = eval_on(proc_test_orig, trainer_mix_all4, "test_orig")

# (B) 변형된 test도 robustness 참고용으로 같이 확인하고 싶다면:
test_raw_16k = test_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
def make_processed_with_rate(ds_raw, rate: float):
    def _aug(b):
        a = b["audio"]
        aug_np, sr_out = speed_perturb_np(a["array"], a["sampling_rate"], rate)
        b["audio"] = {"array": aug_np, "path": None, "sampling_rate": sr_out}
        return b
    ds_aug = ds_raw.map(_aug)
    ds_proc = ds_aug.map(prepare_dataset, remove_columns=[c for c in ds_aug.column_names if c not in {"input_features","labels","locale"}])
    ds_proc.set_format(columns=["input_features","labels","locale"])
    return ds_proc

rates = [0.7, 0.9, 1.1, 1.3]
dfs = [df_orig]
for r in rates:
    ds_r = make_processed_with_rate(test_raw_16k, r)
    df_r = eval_on(ds_r, trainer_mix_all4, f"test_rate{r}")
    dfs.append(df_r)

# 표 출력 & 저장
from IPython.display import display
for d in dfs: display(d)
df_all = pd.concat(dfs, ignore_index=True)
df_all.to_csv("/content/results_mix_all4_test_orig_and_rates.csv", index=False)
print("Saved -> /content/results_mix_all4_test_orig_and_rates.csv")

import matplotlib.pyplot as plt

def bar_lang(df, label):
    plt.figure(figsize=(7,5))
    langs = ["ko","ja","en","de"]
    vals = [df[df["lang"]==l]["WER"].values[0] for l in langs]
    plt.bar([l.upper() for l in langs], vals)
    plt.title(f"WER - {label}"); plt.ylabel("WER"); plt.tight_layout(); plt.show()

# 원본 test 시각화
bar_lang(df_orig, "Original Test")

# 참고용: 변형 test 시각화
for r in [0.7, 0.9, 1.1, 1.3]:
    df_r = df_all[df_all["set"] == f"test_rate{r}"].copy()
    bar_lang(df_r, f"Rate {r} Test")

# 학습 직전 sanity check
print("train mix size:", len(proc_train_mix))
print("val (orig) size:", len(proc_val_orig))
print("first keys train:", proc_train_mix.features)
print("training on:", trainer_mix_all4.args.output_dir)

# 반드시 train_dataset가 proc_train_mix 인지 확인
assert trainer_mix_all4.train_dataset is proc_train_mix
# 검증은 원본만 쓰는지 확인
assert trainer_mix_all4.eval_dataset is proc_val_orig