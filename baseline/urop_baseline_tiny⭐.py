# -*- coding: utf-8 -*-
"""UROP_baseline_tiny⭐.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EfkPSl9pY3kXQELmlXCPTTOzkxDpn9gL
"""

# 핵심 패키지(스트리밍 호환 버전)
!pip -q install "datasets<3.0.0" "evaluate<0.5.0" "transformers>=4.38,<4.45" \
                huggingface_hub torchaudio jiwer soundfile

# fsspec/gcsfs 충돌 정리
!pip -q install -U fsspec==2025.3.0 gcsfs

import datasets, evaluate, transformers, fsspec, gcsfs, sys
print("datasets:", datasets.__version__)    # ← 2.x 여야 OK
print("evaluate:", evaluate.__version__)    # ← 0.4.x 권장
print("transformers:", transformers.__version__)
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("python:", sys.version)

from huggingface_hub import notebook_login
notebook_login()

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

from datasets import load_dataset, Audio, Dataset, load_from_disk
from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
from evaluate import load
import random
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union
from evaluate import load as load_metric
import pandas as pd
import numpy as np

# 2) 재현성 고정
import os, random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

# 결정적 연산(권장)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Seed fixed to {}.".format(SEED))

n_train = 200
n_val   = 100
n_test  = 100

langs = ["ko", "ja", "en", "de"]

# =========================================================
# (중요) 고정 split만 로드: 증강 실험과 동일 테스트셋 사용
# =========================================================
SPLIT_DIR = "/content/drive/MyDrive/data/asr_splits/cv16_v1"  # 네가 저장해 둔 경로
required = ["train","val","test"]
if not (os.path.isdir(SPLIT_DIR) and all(os.path.isdir(os.path.join(SPLIT_DIR, d)) for d in required)):
    raise RuntimeError(f"[STOP] Fixed splits not found under {SPLIT_DIR} (need subdirs: train/ val/ test/).")

print("✅ Loading fixed splits from disk:", SPLIT_DIR)
train_dataset = load_from_disk(os.path.join(SPLIT_DIR, "train"))
val_dataset   = load_from_disk(os.path.join(SPLIT_DIR, "val"))
test_dataset  = load_from_disk(os.path.join(SPLIT_DIR, "test"))
print({ "train": len(train_dataset), "val": len(val_dataset), "test": len(test_dataset) })

model_checkpoint = "openai/whisper-tiny"

processor = WhisperProcessor.from_pretrained(model_checkpoint)
feature_extractor = processor.feature_extractor
tokenizer = processor.tokenizer

# 16kHz 캐스팅
train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=16000))
val_dataset   = val_dataset.cast_column("audio",   Audio(sampling_rate=16000))
test_dataset  = test_dataset.cast_column("audio",  Audio(sampling_rate=16000))

def prepare_dataset(example):
    audio = example["audio"]
    example["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=16000
    ).input_features[0]
    example["labels"] = processor.tokenizer(example["sentence"]).input_ids
    return example

keep_cols = {"input_features", "labels", "locale"}

processed_dataset_train = train_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in train_dataset.column_names if c not in keep_cols]
)
processed_dataset_val = val_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in val_dataset.column_names if c not in keep_cols]
)
processed_dataset_test = test_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in test_dataset.column_names if c not in keep_cols]
)

processed_dataset_train.set_format(columns=["input_features", "labels", "locale"])
processed_dataset_val.set_format(columns=["input_features", "labels", "locale"])
processed_dataset_test.set_format(columns=["input_features", "labels", "locale"])

import torch
from torch import nn
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint)
# 학습 중 캐시 비활성(메모리/안정성)
model.config.use_cache = False
# 평가 때 경고 줄이기용(안전)
start_id = processor.tokenizer.convert_tokens_to_ids("<|startoftranscript|>")
model.config.decoder_start_token_id = start_id
model.generation_config.decoder_start_token_id = start_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.generation_config.pad_token_id = processor.tokenizer.pad_token_id
# 학습 중 언어 강제 OFF (평가 시만 ON)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []

from dataclasses import dataclass

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-tiny-finetuned",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=5e-5,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=50,
    fp16=torch.cuda.is_available(),
    report_to="none",

    predict_with_generate=True,    # ★ generate() 기반 평가
    generation_max_length=225,
    # 아래 3개 켜면 epoch별 평가/저장/베스트모델 사용 가능(그래프용)
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,

    remove_unused_columns=False,   # input_features 보존
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset_train,
    eval_dataset=processed_dataset_val,   # ← 학습 중 평가는 여전히 VAL
    tokenizer=processor,
    data_collator=data_collator,
)

trainer.train()

trainer.save_model("./whisper-tiny-finetuned")
processor.save_pretrained("./whisper-tiny-finetuned")

test_ko = processed_dataset_test.filter(lambda x: x["locale"] == "ko")
test_en = processed_dataset_test.filter(lambda x: x["locale"] == "en")
test_ja = processed_dataset_test.filter(lambda x: x["locale"] == "ja")
test_de = processed_dataset_test.filter(lambda x: x["locale"] == "de")

lang_datasets = {"ko": test_ko, "en": test_en, "ja": test_ja, "de": test_de}
print("🔎 Evaluating on: TEST split (fixed)")

wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

LANG_NAME = {"ko":"korean", "ja":"japanese", "en":"english", "de":"german"}

lang_scores = {}
for lang, dataset in lang_datasets.items():
    print(f"\n🌍 Language: {lang.upper()}")
    # 언어 강제 프롬프트 → 인퍼런스 안정화
    forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
    trainer.model.generation_config.forced_decoder_ids = forced_ids
    trainer.model.generation_config.max_length = 225

    pred_output = trainer.predict(dataset)  # generate 결과 반환
    decoded_preds = processor.tokenizer.batch_decode(pred_output.predictions, skip_special_tokens=True)
    decoded_labels = processor.tokenizer.batch_decode(pred_output.label_ids,   skip_special_tokens=True)

    # 샘플 출력
    for p, r in list(zip(decoded_preds, decoded_labels))[:3]:
        print(f"🔹Pred: {p}\n🔸Ref : {r}\n---")

    wer_score = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)
    cer_score = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)
    lang_scores[lang] = {"WER": wer_score, "CER": cer_score, "N": len(decoded_labels)}

    print(f"✅ {lang.upper()} WER: {wer_score:.4f}")
    print(f"✅ {lang.upper()} CER: {cer_score:.4f}")

import pandas as pd
df_base = pd.DataFrame(
    [{"lang": k, "WER": v["WER"], "CER": v["CER"], "N": v["N"]} for k, v in lang_scores.items()]
).sort_values("lang")
display(df_base)
df_base.to_csv("/content/baseline_cv16_1_whisper_tiny_FIXEDSPLIT_TEST.csv", index=False)
print("Saved -> /content/baseline_cv16_1_whisper_tiny_FIXEDSPLIT_TEST.csv")

# 시각화
import matplotlib.pyplot as plt

languages = ["Korean", "Japanese", "English", "German"]
wer = [df_base[df_base["lang"]==k]["WER"].values[0] for k in ["ko","ja","en","de"]]
cer = [df_base[df_base["lang"]==k]["CER"].values[0] for k in ["ko","ja","en","de"]]

x = np.arange(len(languages))  # 언어 개수만큼 위치
width = 0.35  # 막대 너비

plt.figure(figsize=(8,6))
plt.bar(x - width/2, wer, width, label="WER", color="#6A5ACD")
plt.bar(x + width/2, cer, width, label="CER", color="#ff7f0e")
plt.xticks(x, languages, fontsize=12)
plt.ylabel("Error Rate", fontsize=12)
plt.title("Language-wise WER vs CER (Fixed Split)")
plt.legend()
for i in range(len(languages)):
    plt.text(x[i] - width/2, wer[i] + 0.01, f"{wer[i]:.2f}", ha="center", fontsize=10)
    plt.text(x[i] + width/2, cer[i] + 0.01, f"{cer[i]:.2f}", ha="center", fontsize=10)
plt.tight_layout()
plt.show()