# -*- coding: utf-8 -*-
"""UROP_baselinecode_base(튜닝).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R3tUVyuvQEzf5KF9vo93bY1NGlDFyfJD
"""

# 핵심 패키지(스트리밍 호환 버전)
!pip -q install "datasets<3.0.0" "evaluate<0.5.0" "transformers>=4.38,<4.45" \
                huggingface_hub torchaudio jiwer soundfile

# fsspec/gcsfs 충돌 정리
!pip -q install -U fsspec==2025.3.0 gcsfs

import datasets, evaluate, transformers, fsspec, gcsfs, sys
print("datasets:", datasets.__version__)    # ← 2.x 여야 OK
print("evaluate:", evaluate.__version__)    # ← 0.4.x 권장
print("transformers:", transformers.__version__)
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("python:", sys.version)

from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset, Audio, Dataset
from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
from evaluate import load
import random
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union
from evaluate import load as load_metric
import pandas as pd

# 2) 재현성 고정

import os, random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

# 결정적 연산(권장)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Seed fixed to {}.".format(SEED))

n_train = 200
n_val   = 100
n_test  = 100

langs = ["ko", "ja", "en", "de"]
train_data, val_data, test_data = [], [], []

def take_valid(gen, n):
    out = []
    for ex in gen:
        if ex.get("audio") is not None and ex.get("sentence"):
            out.append(ex)
            if len(out) >= n: break
    return out

for lang in langs:
    print(f"✅ Loading {lang}...")
    ds_tr = load_dataset("mozilla-foundation/common_voice_16_1", lang,
                         split="train", streaming=True)
    ds_va = load_dataset("mozilla-foundation/common_voice_16_1", lang,
                         split="validation", streaming=True)
    ds_te = load_dataset("mozilla-foundation/common_voice_16_1", lang,
                         split="test", streaming=True)

    train_data += take_valid(ds_tr, n_train)
    val_data   += take_valid(ds_va, n_val)
    test_data  += take_valid(ds_te, n_test)

# 셔플(시드 고정됨)
random.shuffle(train_data); random.shuffle(val_data); random.shuffle(test_data)

train_dataset = Dataset.from_list(train_data)
val_dataset   = Dataset.from_list(val_data)
test_dataset  = Dataset.from_list(test_data)
print({ "train": len(train_dataset), "val": len(val_dataset), "test": len(test_dataset) })

model_checkpoint = "openai/whisper-base"

processor = WhisperProcessor.from_pretrained(model_checkpoint)
feature_extractor = processor.feature_extractor
tokenizer = processor.tokenizer

# 16kHz 캐스팅
train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=16000))
val_dataset   = val_dataset.cast_column("audio",   Audio(sampling_rate=16000))
test_dataset  = test_dataset.cast_column("audio",  Audio(sampling_rate=16000))

def prepare_dataset(example):
    audio = example["audio"]
    example["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=16000
    ).input_features[0]
    example["labels"] = processor.tokenizer(example["sentence"]).input_ids
    return example

keep_cols = {"input_features", "labels", "locale"}

processed_dataset_train = train_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in train_dataset.column_names if c not in keep_cols],
    load_from_cache_file=False,
    desc="prep_train"
)
processed_dataset_val = val_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in val_dataset.column_names if c not in keep_cols],
    load_from_cache_file=False,
    desc="prep_val"
)
processed_dataset_test = test_dataset.map(
    prepare_dataset,
    remove_columns=[c for c in test_dataset.column_names if c not in keep_cols],
    load_from_cache_file=False,
    desc="prep_test"
)

processed_dataset_train.set_format(columns=["input_features", "labels", "locale"])
processed_dataset_val.set_format(columns=["input_features", "labels", "locale"])
processed_dataset_test.set_format(columns=["input_features", "labels", "locale"])

import torch
from torch import nn
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint)
# 학습 안정성/메모리
model.config.use_cache = False
# 학습 중 언어 강제 끔(평가 때만 사용)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
# 큰 모델 안정화
try:
    model.gradient_checkpointing_enable()
except Exception as e:
    print("gradient checkpointing not enabled:", e)

from dataclasses import dataclass

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # add decoder_input_ids
        decoder_input_ids = labels[:, :-1].clone()
        decoder_input_ids[decoder_input_ids == -100] = self.processor.tokenizer.pad_token_id
        batch["decoder_input_ids"] = decoder_input_ids

        batch["labels"] = labels[:, 1:]
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

start_id = processor.tokenizer.convert_tokens_to_ids("<|startoftranscript|>")
model.config.decoder_start_token_id = start_id
model.generation_config.decoder_start_token_id = start_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
# (선택) 방어적 체크
assert model.config.decoder_start_token_id is not None
assert model.config.pad_token_id is not None

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback

# ✅ Whisper-base 권장 튜닝(epochs→steps, 작은 LR, smoothing 0.05)
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-base-finetuned",
    per_device_train_batch_size=4,      # VRAM에 맞게 조절 가능
    gradient_accumulation_steps=2,      # eff_batch ≈ 8
    learning_rate=7e-6,                 # base 권장 범위(5e-6~1e-5)
    max_steps=4000,                     # 총 업데이트 수 고정(공정 비교)
    warmup_ratio=0.1,
    weight_decay=0.01,
    label_smoothing_factor=0.05,

    logging_steps=100,
    fp16=torch.cuda.is_available(),
    report_to="none",

    predict_with_generate=True,
    generation_max_length=225,

    evaluation_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=200,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    remove_unused_columns=False,
    data_seed=SEED,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset_train,
    eval_dataset=processed_dataset_val,
    tokenizer=processor,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],  # base는 느려서 여유
)

trainer.train()

trainer.save_model("./whisper-base-finetuned")
processor.save_pretrained("./whisper-base-finetuned")

# ── 평가(언어별): VAL
val_ko = processed_dataset_val.filter(lambda x: x["locale"] == "ko")
val_en = processed_dataset_val.filter(lambda x: x["locale"] == "en")
val_ja = processed_dataset_val.filter(lambda x: x["locale"] == "ja")
val_de = processed_dataset_val.filter(lambda x: x["locale"] == "de")

lang_datasets = {"ko": val_ko, "en": val_en, "ja": val_ja, "de": val_de}

wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

LANG_NAME = {"ko":"korean", "ja":"japanese", "en":"english", "de":"german"}

lang_scores = {}
for lang, dataset in lang_datasets.items():
    print(f"\n🌍 Language: {lang.upper()}")
    # 평가 시에만 언어 프롬프트 강제
    forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
    trainer.model.generation_config.forced_decoder_ids = forced_ids
    trainer.model.generation_config.max_length = 225

    pred_output = trainer.predict(dataset)
    decoded_preds = processor.tokenizer.batch_decode(pred_output.predictions, skip_special_tokens=True)
    decoded_labels = processor.tokenizer.batch_decode(pred_output.label_ids,   skip_special_tokens=True)

    # 샘플 출력
    for p, r in list(zip(decoded_preds, decoded_labels))[:10]:
        print(f"🔹Pred: {p}\n🔸Ref : {r}\n---")

    wer_score = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)
    cer_score = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)
    lang_scores[lang] = {"WER": wer_score, "CER": cer_score, "N": len(decoded_labels)}

    print(f"✅ {lang.upper()} WER: {wer_score:.4f}")
    print(f"✅ {lang.UPPER() if hasattr(str, 'UPPER') else lang.upper()} CER: {cer_score:.4f}")

# 표/저장
df_base = pd.DataFrame(
    [{"lang": k, "WER": v["WER"], "CER": v["CER"], "N": v["N"]} for k, v in lang_scores.items()]
).sort_values("lang")
display(df_base)
df_base.to_csv("/content/baseline_cv16_1_whisper_base_tuned.csv", index=False)
print("Saved -> /content/baseline_cv16_1_whisper_base_tuned.csv")

# 매크로 평균도 같이 출력
print("\nMacro averages (VAL):")
print("WER:", df_base["WER"].mean(), "CER:", df_base["CER"].mean())

# ── 시각화(자동: df에서 값 읽기)
import matplotlib.pyplot as plt
import numpy as np

languages = [l.upper() for l in df_base["lang"].tolist()]
wer_vals = df_base["WER"].to_numpy()
cer_vals = df_base["CER"].to_numpy()

x = np.arange(len(languages))
width = 0.35

plt.figure(figsize=(8,6))
plt.bar(x - width/2, wer_vals, width, label="WER")
plt.bar(x + width/2, cer_vals, width, label="CER")
plt.xticks(x, languages, fontsize=12)
plt.ylabel("Error Rate", fontsize=12)
plt.title("Language-wise WER vs CER (VAL)", fontsize=14, fontweight="bold")
plt.legend()
for i in range(len(languages)):
    plt.text(x[i] - width/2, wer_vals[i] + 0.01, f"{wer_vals[i]:.2f}", ha="center", fontsize=10)
    plt.text(x[i] + width/2, cer_vals[i] + 0.01, f"{cer_vals[i]:.2f}", ha="center", fontsize=10)
plt.tight_layout()
plt.show()