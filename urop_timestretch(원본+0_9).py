# -*- coding: utf-8 -*-
"""UROP_TimeStretch(원본+0.9).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxQjDizcHsmFPy-Jys8Z5Ptekw4NXzbX
"""

# 핵심 패키지(스트리밍 호환 버전)
!pip -q install "datasets<3.0.0" "evaluate<0.5.0" "transformers>=4.38,<4.45" \
                huggingface_hub torchaudio jiwer soundfile

# fsspec/gcsfs 충돌 정리
!pip -q install -U fsspec==2025.3.0 gcsfs

import datasets, evaluate, transformers, fsspec, gcsfs, sys
print("datasets:", datasets.__version__)    # ← 2.x 여야 OK
print("evaluate:", evaluate.__version__)    # ← 0.4.x 권장
print("transformers:", transformers.__version__)
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("python:", sys.version)

from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset, Audio, Dataset, concatenate_datasets
from transformers import (
    WhisperProcessor, WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback
)
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from evaluate import load as load_metric
import pandas as pd

import random
import torch

# 2) 재현성 고정

import os, random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

# 결정적 연산(권장)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Seed fixed to {}.".format(SEED))

n_train, n_val, n_test = 200, 100, 100
langs = ["ko","ja","en","de"]

def take_valid(gen, n):
    out = []
    for ex in gen:
        if ex.get("audio") is not None and ex.get("sentence"):
            out.append(ex)
            if len(out) >= n: break
    return out

train_data, val_data, test_data = [], [], []
for lang in langs:
    print(f"✅ Loading {lang}...")
    ds_tr = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="train",       streaming=True)
    ds_va = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="validation",  streaming=True)
    ds_te = load_dataset("mozilla-foundation/common_voice_16_1", lang, split="test",        streaming=True)
    train_data += take_valid(ds_tr, n_train)
    val_data   += take_valid(ds_va, n_val)
    test_data  += take_valid(ds_te, n_test)

random.shuffle(train_data); random.shuffle(val_data); random.shuffle(test_data)
train_dataset = Dataset.from_list(train_data)
val_dataset   = Dataset.from_list(val_data)
test_dataset  = Dataset.from_list(test_data)
print({"train": len(train_dataset), "val": len(val_dataset), "test": len(test_dataset)})

TARGET_SR = 16000
model_checkpoint = "openai/whisper-tiny"

processor = WhisperProcessor.from_pretrained(model_checkpoint)
feature_extractor = processor.feature_extractor
tokenizer = processor.tokenizer

train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
val_dataset   = val_dataset.cast_column("audio",   Audio(sampling_rate=TARGET_SR))
test_dataset  = test_dataset.cast_column("audio",  Audio(sampling_rate=TARGET_SR))

def prepare_dataset(example):
    a = example["audio"]
    example["input_features"] = processor.feature_extractor(a["array"], sampling_rate=TARGET_SR).input_features[0]
    example["labels"] = processor.tokenizer(example["sentence"]).input_ids
    return example

keep = {"input_features","labels","locale"}
proc_train_orig = train_dataset.map(prepare_dataset, remove_columns=[c for c in train_dataset.column_names if c not in keep])
proc_val_orig   = val_dataset.map(prepare_dataset,   remove_columns=[c for c in val_dataset.column_names   if c not in keep])
proc_test_orig  = test_dataset.map(prepare_dataset,  remove_columns=[c for c in test_dataset.column_names  if c not in keep])

proc_train_orig.set_format(columns=list(keep))
proc_val_orig.set_format(columns=list(keep))
proc_test_orig.set_format(columns=list(keep))

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device, "| Seed:", SEED)

model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint).to(device)
model.config.use_cache = False  # 학습 안정성
# 학습 중 언어 강제 끄기 (중요)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    padding: Union[bool, str] = True
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

!apt-get -y install sox libsox-dev libsox-fmt-all

import numpy as np, torchaudio, torch

def _to_tensor_1ch(x_np):
    x = torch.tensor(np.asarray(x_np, dtype=np.float32))
    if x.dim() == 1:
        x = x.unsqueeze(0)
    return x

def speed_perturb_np(wav_np, sr, rate: float):
    if rate == 1.0:
        return wav_np, sr
    wav = _to_tensor_1ch(wav_np)
    effects = [["speed", str(rate)], ["rate", str(sr)]]
    aug, sr_out = torchaudio.sox_effects.apply_effects_tensor(wav, sr, effects)
    return aug.squeeze(0).numpy(), sr_out

# 원본 raw(16k 보장)로부터 증강 사본 생성 → prepare_dataset 재사용
train_raw_16k = train_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))

def augment_copy(ds_raw, rate: float):
    def _aug(b):
        a = b["audio"]
        aug_np, sr_out = speed_perturb_np(a["array"], a["sampling_rate"], rate)
        b["audio"] = {"array": aug_np, "path": None, "sampling_rate": sr_out}
        return b
    ds_aug = ds_raw.map(_aug)
    ds_proc = ds_aug.map(
        prepare_dataset,
        remove_columns=[c for c in ds_aug.column_names if c not in {"input_features","labels","locale"}]
    )
    ds_proc.set_format(columns=["input_features","labels","locale"])
    return ds_proc

# ★ 이번 실험: 원본 + 0.9 (1:1)
proc_train_sp09 = augment_copy(train_raw_16k, 0.9)

# 원본 길이에 맞춰 1:1로 합치기  (❗select에 정수 대신 range 사용)
k = len(proc_train_orig)
m = min(len(proc_train_sp09), k)  # 사용할 증강 샘플 수
proc_train_sp09_sel = proc_train_sp09.shuffle(seed=SEED).select(range(m))

# (선택) 정말 1:1 정확히 맞추려면 원본도 m개로 맞춤
# proc_train_orig_sel = proc_train_orig.shuffle(seed=SEED).select(range(m))
# proc_train_mix = concatenate_datasets([proc_train_orig_sel, proc_train_sp09_sel]).shuffle(seed=SEED)

# 원본 전체 + 증강 m개 (원본이 더 많아도 OK)
proc_train_mix = concatenate_datasets([proc_train_orig, proc_train_sp09_sel]).shuffle(seed=SEED)

print(
    "Train(orig):", len(proc_train_orig),
    "Train(0.9 sel):", len(proc_train_sp09_sel),
    "→ Mixed:", len(proc_train_mix)
)

# ---- 모델/생성 설정 (Trainer 전에) ----
start_id = processor.tokenizer.convert_tokens_to_ids("<|startoftranscript|>")
model.config.decoder_start_token_id = start_id
model.generation_config.decoder_start_token_id = start_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.use_cache = False
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback

args = Seq2SeqTrainingArguments(
    output_dir="./whisper-tiny-mix_0p9",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2e-5,
    num_train_epochs=8,
    weight_decay=0.01,
    label_smoothing_factor=0.1,     # 먼저 0.0으로 (원하면 나중에 0.1로 재시도)
    warmup_ratio=0.1,
    lr_scheduler_type="linear",
    max_grad_norm=1.0,
    fp16=torch.cuda.is_available(),
    report_to="none",

    predict_with_generate=True,
    generation_max_length=225,

    eval_strategy="epoch",          # ← deprecation 대응
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    remove_unused_columns=False,    # ← 중요: input_features 보존
)

trainer_mix09 = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=proc_train_mix,
    eval_dataset=proc_val_orig,
    tokenizer=processor,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

LANG_NAME = {"ko":"korean", "ja":"japanese", "en":"english", "de":"german"}
wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

# (A) 원본 test 평가
def eval_on(ds_proc, trainer_obj, label):
    rows = []
    for lang in ["ko","ja","en","de"]:
        subset = ds_proc.filter(lambda x: x["locale"] == lang)
        forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
        trainer_obj.model.generation_config.forced_decoder_ids = forced_ids
        trainer_obj.model.generation_config.max_length = 225
        out = trainer_obj.predict(subset)
        preds = processor.tokenizer.batch_decode(out.predictions, skip_special_tokens=True)
        refs  = processor.tokenizer.batch_decode(out.label_ids,   skip_special_tokens=True)
        wer = wer_metric.compute(predictions=preds, references=refs)
        cer = cer_metric.compute(predictions=preds, references=refs)
        rows.append({"set": label, "lang": lang, "WER": wer, "CER": cer, "N": len(refs)})
    return pd.DataFrame(rows).sort_values("lang")

df_orig = eval_on(proc_test_orig, trainer_mix09, "test_orig")

# (B) 0.9배로 변형된 test 생성 → 평가
test_raw_16k = test_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
def make_processed_with_rate(ds_raw, rate: float):
    def _aug(b):
        a = b["audio"]
        aug_np, sr_out = speed_perturb_np(a["array"], a["sampling_rate"], rate)
        b["audio"] = {"array": aug_np, "path": None, "sampling_rate": sr_out}
        return b
    ds_aug = ds_raw.map(_aug)
    ds_proc = ds_aug.map(prepare_dataset, remove_columns=[c for c in ds_aug.column_names if c not in {"input_features","labels","locale"}])
    ds_proc.set_format(columns=["input_features","labels","locale"])
    return ds_proc

proc_test_r09 = make_processed_with_rate(test_raw_16k, 0.9)
df_r09   = eval_on(proc_test_r09, trainer_mix09, "test_rate0.9")

display(df_orig, df_r09)
df_all = pd.concat([df_orig, df_r09], ignore_index=True)
df_all.to_csv("/content/results_mix0p9_test_orig_and_r09.csv", index=False)
print("Saved -> /content/results_mix0p9_test_orig_and_r09.csv")

import matplotlib.pyplot as plt

def bar_lang(df, label):
    plt.figure(figsize=(7,5))
    langs = ["ko","ja","en","de"]
    vals = [df[df["lang"]==l]["WER"].values[0] for l in langs]
    plt.bar([l.upper() for l in langs], vals)
    plt.title(f"WER - {label}"); plt.ylabel("WER"); plt.tight_layout(); plt.show()

bar_lang(df_orig, "Original Test")
bar_lang(df_r09,  "Rate 0.9 Test")

# 학습 직전 sanity check
print("train mix size:", len(proc_train_mix))
print("val (orig) size:", len(proc_val_orig))
print("first keys train:", proc_train_mix.features)
print("training on:", trainer_mix09.args.output_dir)

# 반드시 train_dataset가 proc_train_mix 인지 확인
assert trainer_mix09.train_dataset is proc_train_mix
# 검증은 원본만 쓰는지 확인
assert trainer_mix09.eval_dataset is proc_val_orig

"""#==================================================
일단 불필요한 셀
"""

from datasets import concatenate_datasets

# 1) 원본 test(이미 있다면 재생성 불필요)
# proc_test_orig 가 이미 있다면 이 블록은 생략 가능
test_raw_16k = test_dataset.cast_column("audio", Audio(sampling_rate=TARGET_SR))
proc_test_orig = test_raw_16k.map(
    prepare_dataset,
    remove_columns=[c for c in test_raw_16k.column_names if c not in {"input_features","labels","locale"}]
)
proc_test_orig.set_format(columns=["input_features","labels","locale"])

# 2) 0.9배 test 생성(이미 있다면 재사용)
proc_test_r09 = make_processed_with_rate(test_raw_16k, 0.9)

# 3) 합치기 → 하나의 혼합 test
proc_test_mix = concatenate_datasets([proc_test_orig, proc_test_r09]).shuffle(seed=SEED)

# 4) 언어별 성능 (한 번에)
df_mix = eval_on(proc_test_mix, trainer_mix09, "test_orig+rate0.9")
display(df_mix)
df_mix.to_csv("/content/results_mix0p9_TEST_mixed_by_lang.csv", index=False)
print("Saved -> /content/results_mix0p9_TEST_mixed_by_lang.csv")

# 5) (선택) 전체 마이크로 평균 WER/CER (언어 합산)
def eval_overall_mic(av_ds, trainer_obj, label):
    preds_all, refs_all = [], []
    for lang in ["ko","ja","en","de"]:
        subset = av_ds.filter(lambda x: x["locale"] == lang)
        if len(subset)==0:
            continue
        forced_ids = processor.get_decoder_prompt_ids(language=LANG_NAME[lang], task="transcribe")
        trainer_obj.model.generation_config.forced_decoder_ids = forced_ids
        trainer_obj.model.generation_config.max_length = 225
        out = trainer_obj.predict(subset)
        preds_all += processor.tokenizer.batch_decode(out.predictions, skip_special_tokens=True)
        refs_all  += processor.tokenizer.batch_decode(out.label_ids,   skip_special_tokens=True)
    return {
        "set": label,
        "WER": wer_metric.compute(predictions=preds_all, references=refs_all),
        "CER": cer_metric.compute(predictions=preds_all, references=refs_all),
        "N": len(refs_all),
    }

overall = eval_overall_mic(proc_test_mix, trainer_mix09, "mixed_micro")
print(overall)  # {'set': 'mixed_micro', 'WER': ..., 'CER': ..., 'N': ...}